{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume($)</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Daily Return</th>\n",
       "      <th>SMA_30</th>\n",
       "      <th>SMA_60</th>\n",
       "      <th>...</th>\n",
       "      <th>Close_rolling_std_7</th>\n",
       "      <th>Volume_rolling_mean_7</th>\n",
       "      <th>Volume_rolling_std_7</th>\n",
       "      <th>Close_rolling_mean_30</th>\n",
       "      <th>Close_rolling_std_30</th>\n",
       "      <th>Volume_rolling_mean_30</th>\n",
       "      <th>Volume_rolling_std_30</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28994.009766</td>\n",
       "      <td>29600.626953</td>\n",
       "      <td>28803.585938</td>\n",
       "      <td>29374.152344</td>\n",
       "      <td>29374.152344</td>\n",
       "      <td>40730301359</td>\n",
       "      <td>BTC</td>\n",
       "      <td>0.012842</td>\n",
       "      <td>22428.243750</td>\n",
       "      <td>19715.608773</td>\n",
       "      <td>...</td>\n",
       "      <td>1284.059118</td>\n",
       "      <td>4.970113e+10</td>\n",
       "      <td>8.115395e+09</td>\n",
       "      <td>22428.243750</td>\n",
       "      <td>3661.566866</td>\n",
       "      <td>3.886562e+10</td>\n",
       "      <td>1.223869e+10</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29376.455078</td>\n",
       "      <td>33155.117188</td>\n",
       "      <td>29091.181641</td>\n",
       "      <td>32127.267578</td>\n",
       "      <td>32127.267578</td>\n",
       "      <td>67865420765</td>\n",
       "      <td>BTC</td>\n",
       "      <td>0.093726</td>\n",
       "      <td>22850.972721</td>\n",
       "      <td>20018.558219</td>\n",
       "      <td>...</td>\n",
       "      <td>1936.633106</td>\n",
       "      <td>5.249153e+10</td>\n",
       "      <td>1.055716e+10</td>\n",
       "      <td>22850.972721</td>\n",
       "      <td>4019.854985</td>\n",
       "      <td>4.006346e+10</td>\n",
       "      <td>1.325301e+10</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32129.408203</td>\n",
       "      <td>34608.558594</td>\n",
       "      <td>32052.316406</td>\n",
       "      <td>32782.023438</td>\n",
       "      <td>32782.023438</td>\n",
       "      <td>78665235202</td>\n",
       "      <td>BTC</td>\n",
       "      <td>0.020380</td>\n",
       "      <td>23320.381315</td>\n",
       "      <td>20329.363493</td>\n",
       "      <td>...</td>\n",
       "      <td>2189.909798</td>\n",
       "      <td>5.423229e+10</td>\n",
       "      <td>1.376529e+10</td>\n",
       "      <td>23320.381315</td>\n",
       "      <td>4328.735995</td>\n",
       "      <td>4.155655e+10</td>\n",
       "      <td>1.494648e+10</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Open          High           Low         Close     Adj Close  \\\n",
       "0  28994.009766  29600.626953  28803.585938  29374.152344  29374.152344   \n",
       "1  29376.455078  33155.117188  29091.181641  32127.267578  32127.267578   \n",
       "2  32129.408203  34608.558594  32052.316406  32782.023438  32782.023438   \n",
       "\n",
       "     Volume($) Symbol  Daily Return        SMA_30        SMA_60  ...  \\\n",
       "0  40730301359    BTC      0.012842  22428.243750  19715.608773  ...   \n",
       "1  67865420765    BTC      0.093726  22850.972721  20018.558219  ...   \n",
       "2  78665235202    BTC      0.020380  23320.381315  20329.363493  ...   \n",
       "\n",
       "   Close_rolling_std_7  Volume_rolling_mean_7  Volume_rolling_std_7  \\\n",
       "0          1284.059118           4.970113e+10          8.115395e+09   \n",
       "1          1936.633106           5.249153e+10          1.055716e+10   \n",
       "2          2189.909798           5.423229e+10          1.376529e+10   \n",
       "\n",
       "   Close_rolling_mean_30  Close_rolling_std_30  Volume_rolling_mean_30  \\\n",
       "0           22428.243750           3661.566866            3.886562e+10   \n",
       "1           22850.972721           4019.854985            4.006346e+10   \n",
       "2           23320.381315           4328.735995            4.155655e+10   \n",
       "\n",
       "   Volume_rolling_std_30  Year  Month  Day  \n",
       "0           1.223869e+10  2021      1    1  \n",
       "1           1.325301e+10  2021      1    2  \n",
       "2           1.494648e+10  2021      1    3  \n",
       "\n",
       "[3 rows x 42 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('data/df_ml.csv', sep=\",\")\n",
    "#df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "#df.drop(index=1, inplace=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Close','Symbol', 'Close_lag_1', 'Low','Midpoint','High','Open','Close_lag_3']] #\n",
    "#df = df[['Close','Symbol','Close_lag_7','EMA_90', '90_day_MA', 'Close_rolling_mean_30','EMA_60','EMA_26','SMA_30']]\n",
    "#df.head(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "28             Close_lag_7        1\n",
    "12                  EMA_90        1\n",
    "23               90_day_MA        1\n",
    "34   Close_rolling_mean_30        1\n",
    "11                  EMA_60        2\n",
    "15                  EMA_26        3\n",
    "7                   SMA_30        4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Onehot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "\n",
    "artists_column = df['Symbol'].values.reshape(-1, 1)\n",
    "\n",
    "one_hot_encoded_artists = one_hot_encoder.fit_transform(artists_column)\n",
    "\n",
    "df['Symbol'] = np.argmax(one_hot_encoded_artists, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling and Spliting train,test,validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split data into features (X) and target variable (y)\n",
    "X = df.drop('Close', axis=1)  \n",
    "y = df['Close']\n",
    "\n",
    "# Split the dataset into training (70%), validation (15%), and test (15%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MSE: 19942.26454936525\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Initialize the model\n",
    "linear_reg = LinearRegression()\n",
    "\n",
    "\n",
    "linear_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = linear_reg.predict(X_test_scaled)\n",
    "\n",
    "mse_lr = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Linear Regression MSE: {mse_lr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.9998187610689385\n",
      "Mean Absolute Error (MAE): 30.31806788784291\n",
      "Root Mean Squared Error (RMSE): 141.2170830649226\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = r2_score(y_test, y_pred)\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation for Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Cross-Validated MSE: 21423.14803174241\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Initialize the model\n",
    "linear_reg = LinearRegression()\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores_lr = cross_val_score(linear_reg, X_scaled, y, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Calculate the mean MSE\n",
    "mean_mse_lr = -cv_scores_lr.mean()\n",
    "print(f\"Linear Regression Cross-Validated MSE: {mean_mse_lr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search\n",
    "Grid Search evaluates all the provided combinations of hyperparameters, which can be computationally expensive but thorough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mrf, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Fit the grid search to the data\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Print the best parameters and the best score\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters found: \u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32mc:\\Users\\Ethan Vaz Falcao\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:891\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    885\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    886\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    887\u001b[0m     )\n\u001b[0;32m    889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 891\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    895\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Ethan Vaz Falcao\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1392\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1391\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1392\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ethan Vaz Falcao\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:838\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    831\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    832\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    833\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    834\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    835\u001b[0m         )\n\u001b[0;32m    836\u001b[0m     )\n\u001b[1;32m--> 838\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    857\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    859\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    860\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Ethan Vaz Falcao\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[1;32mc:\\Users\\Ethan Vaz Falcao\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[1;32mc:\\Users\\Ethan Vaz Falcao\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ethan Vaz Falcao\\anaconda3\\lib\\concurrent\\futures\\_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32mc:\\Users\\Ethan Vaz Falcao\\anaconda3\\lib\\threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define the model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Set up the hyperparameters grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Set up the grid search\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best score found: \", -grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
      "Best parameters found:  {'bootstrap': True, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best score found:  38483.10625303709\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # Number of trees in the forest\n",
    "    'max_depth': [10, 20, 30, None],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required at a leaf node\n",
    "    'bootstrap': [True, False]  # Method of selecting samples for training each tree\n",
    "}\n",
    "\n",
    "# Initialize the Grid Search model\n",
    "grid_search = GridSearchCV(estimator=RandomForestRegressor(random_state=42), param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best parameters found by Grid Search\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# Best score found by Grid Search\n",
    "print(\"Best score found: \", -grid_search.best_score_)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "optimized_random_forest = RandomForestRegressor(**best_params, random_state=42)\n",
    "\n",
    "# Fit the model with the training data\n",
    "optimized_random_forest.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_optimized_rf = optimized_random_forest.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model with the test data\n",
    "optimized_mse_rf = mean_squared_error(y_test, y_pred_optimized_rf)\n",
    "print(f\"Optimized Random Forest MSE: {optimized_mse_rf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest MSE with Best Parameters: 33965.6070107671\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Initialize the model with the best parameters from Grid Search\n",
    "random_forest = RandomForestRegressor(\n",
    "    n_estimators=100,  # Best number of trees\n",
    "    max_depth=30,  # Best maximum depth of trees\n",
    "    min_samples_leaf=1,  # Best minimum number of samples required at a leaf node\n",
    "    min_samples_split=2,  # Best minimum number of samples required to split an internal node\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "# Fit the model with the training data\n",
    "random_forest.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_rf = random_forest.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model with the test data\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "print(f\"Random Forest MSE with Best Parameters: {mse_rf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.9996913143794554\n",
      "Mean Absolute Error (MAE): 40.66735938525787\n",
      "Root Mean Squared Error (RMSE): 184.29760446290965\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = r2_score(y_test, y_pred_rf)\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred_rf)\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Cross-Validated MSE: 96922514.69028388\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialize the model\n",
    "random_forest = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores_rf = cross_val_score(random_forest, X_scaled, y, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Calculate the mean MSE\n",
    "mean_mse_rf = -cv_scores_rf.mean()\n",
    "print(f\"Random Forest Cross-Validated MSE: {mean_mse_rf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Machines (GBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Search\n",
    "Random Search samples a given number of candidates from a parameter space with a specified distribution. It's less comprehensive but much faster than Grid Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'subsample': 0.6, 'n_estimators': 1000, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "Best score found:  39292.73854500158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "# Define the parameter distribution to sample from\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'colsample_bytree': [0.3, 0.7],\n",
    "    'subsample': [0.6, 0.9]\n",
    "}\n",
    "\n",
    "# Initialize the XGBRegressor\n",
    "xg_reg = xgb.XGBRegressor()\n",
    "\n",
    "# Setup RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=xg_reg, param_distributions=param_dist, n_iter=25, scoring='neg_mean_squared_error', cv=3, n_jobs=-1, random_state=42)\n",
    "\n",
    "# Fit RandomizedSearchCV\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "print(\"Best score found: \", -random_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized XGBoost MSE: 32797.58750307243\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Adjusting the model with the best parameters found\n",
    "xg_reg_optimized = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    colsample_bytree=0.7,  # Updated from 0.3 to 0.7 based on the best parameters\n",
    "    learning_rate=0.1,     # Remains the same as before\n",
    "    max_depth=5,           # Remains the same as before\n",
    "    alpha=10,              # Remains the same as before\n",
    "    n_estimators=500,      # Updated from 10 to 500 based on the best parameters\n",
    "    subsample=0.8          # Added based on the best parameters\n",
    ")\n",
    "\n",
    "# Fit the model with the adjusted parameters\n",
    "xg_reg_optimized.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_optimized = xg_reg_optimized.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model with the adjusted parameters\n",
    "mse_xgb_optimized = mean_squared_error(y_test, y_pred_optimized)\n",
    "print(f\"Optimized XGBoost MSE: {mse_xgb_optimized}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.9997019295533995\n",
      "Mean Absolute Error (MAE): 39.37663597807381\n",
      "Root Mean Squared Error (RMSE): 181.10104224733888\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = r2_score(y_test, y_pred_optimized)\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred_optimized)\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_optimized))\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Cross-Validated MSE: 98328661.26877618\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Initialize the model\n",
    "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree=0.3, learning_rate=0.1,\n",
    "                max_depth=5, alpha=10, n_estimators=10)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores_xgb = cross_val_score(xg_reg, X_scaled, y, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Calculate the mean MSE\n",
    "mean_mse_xgb = -cv_scores_xgb.mean()\n",
    "print(f\"XGBoost Cross-Validated MSE: {mean_mse_xgb}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LazyPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 31/42 [01:05<00:13,  1.18s/it]"
     ]
    }
   ],
   "source": [
    "from lazypredict.Supervised import LazyRegressor\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Shuffle your dataset (optional but recommended for cross-sectional data)\n",
    "#X, y = shuffle(X, y, random_state=42)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize LazyRegressor\n",
    "reg = LazyRegressor(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "\n",
    "# Fit and compare models\n",
    "models, predictions = reg.fit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Print the performance of each model\n",
    "print(models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/df_fs.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
